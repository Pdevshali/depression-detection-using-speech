ğŸ™ï¸ Voice-Based Depression Detection System

Using Wav2Vec2 Embeddings + RF/XGBoost Ensemble

FastAPI Backend + Streamlit Chatbot Frontend

ğŸ“Œ Overview

This project implements an AI-based system designed to classify whether a speaker is Depressed or Not Depressed by analyzing their vocal features from structured interview recordings.

The core technology stack includes:

Wav2Vec2: Utilized for extracting robust, general-purpose speech embeddings.

Ensemble Classifier: A powerful combination of Random Forest and XGBoost models for final prediction.

FastAPI: Powers a high-performance backend API for processing audio inference requests.

Streamlit: Provides a user-friendly, chatbot-like frontend for interactive testing, supporting both live audio recording and .wav file uploads.

The system is designed to simulate structured AVEC-style interviews where users verbally answer a series of questions.

ğŸ›ï¸ System Architecture

The application is structured into three distinct layers, ensuring separation of concerns and scalability.

Components:

Streamlit UI (Frontend): Manages the user experience, presents chatbot questions, and captures audio input.

FastAPI Backend: Handles API requests (POST /predict), manages audio file transfer, and orchestrates the call to the inference layer.

Inference Layer (inference.py): The core machine learning engine responsible for audio preprocessing, feature extraction (Wav2Vec2), ensemble model prediction, and thresholding.

ğŸ“‚ Project Structure

Dissertation-Project/
â”‚
â”œâ”€â”€ backend/
â”‚ Â  â”œâ”€â”€ backend.py Â  Â  Â  Â  Â # FastAPI server application
â”‚ Â  â”œâ”€â”€ inference.py Â  Â  Â  Â # Wav2Vec2 + ensemble inference logic
â”‚
â”œâ”€â”€ frontend/
â”‚ Â  â”œâ”€â”€ app.py Â  Â  Â  Â  Â  Â  Â # Streamlit chatbot UI
â”‚
â”œâ”€â”€ models_res/
â”‚ Â  â”œâ”€â”€ rf_wav2vec_model.joblib Â  Â # Saved Random Forest model
â”‚ Â  â”œâ”€â”€ xgb_wav2vec_model.joblib Â  # Saved XGBoost model
â”‚ Â  â”œâ”€â”€ ensemble_alpha.npy Â  Â  Â  Â  # Optimal blending weight (alpha)
â”‚ Â  â”œâ”€â”€ ensemble_threshold.npy Â  Â  # Optimal prediction threshold
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md Â  Â  Â  Â  Â  Â  Â  Â # This file


ğŸ”§ Installation & Setup

1ï¸âƒ£ Create Virtual Environment

python -m venv venv


Activate:

# On Windows (CMD)
venv\Scripts\activate

# On Git Bash / Linux / macOS
source venv/bin/activate


2ï¸âƒ£ Install Dependencies

Ensure your environment is active before running:

pip install -r requirements.txt


3ï¸âƒ£ Install FFmpeg (Required for Audio Recording)

FFmpeg is essential for handling audio formats and is required by several libraries used for live recording.

Option A: Using winget (Windows)

winget install Gyan.FFmpeg


ğŸš€ Running the System

â–¶ï¸ Start FastAPI Backend

Navigate to the backend directory and run the Uvicorn server:

cd backend
uvicorn backend:app --reload


The Backend runs at: ğŸ“ http://127.0.0.1:8000

Key Endpoints:

GET / â€“ API health check

POST /predict â€“ Endpoint for audio prediction (accepts audio bytes)

ğŸ–¥ï¸ Start Streamlit Frontend

Navigate to the frontend directory and launch the Streamlit app:

cd frontend
streamlit run app.py


The Frontend opens at: ğŸ“ http://localhost:8501

Frontend Features:

ğŸ¤ Record answers to conversational interview questions.

â¬† Upload a pre-recorded .wav file.

ğŸ§  Provides real-time and aggregated predictions.

ğŸ’¬ Simulates a controlled chatbot conversation flow.

ğŸ§  Model Pipeline

The inference process involves four main stages:

1ï¸âƒ£ Preprocessing

Convert audio channel to mono.

Normalize amplitude.

Resample the audio to the standard rate of 16 kHz.

Trim silence.

Split the input audio into fixed 15-second segments (chunks).

2ï¸âƒ£ Feature Extraction

Utilize the facebook/wav2vec2-base-960h pretrained model.

Compute the mean-pooled embedding from the last hidden state for each chunk, resulting in a 768-dimensional feature vector.

3ï¸âƒ£ Ensemble Classification

Predictions are generated by two distinct models: Random Forest (RF) and XGBoost (XGB).

The final probability (y) is calculated using an optimized blend:


$$y = \alpha \cdot \text{XGB} + (1 - \alpha) \cdot \text{RF}$$

$\alpha$ is the learned ensemble weight (ensemble_alpha.npy).

The class prediction uses an optimized threshold (Youdenâ€™s J) from ensemble_threshold.npy.

4ï¸âƒ£ Participant-Level Decision

The final participant-level prediction is the mean probability across the predictions of all processed 15-second chunks.

ğŸ“Š Model Evaluation (Participant-Level)

Metric

Score

AUC

0.81

Accuracy

0.71

Precision (Depressed)

0.59

Recall (Depressed)

0.79

Note: The use of the ensemble and chunk aggregation approach was crucial in achieving the high Recall score for the Depressed class.

ğŸ§ª Testing the Model Programmatically

The core prediction logic can be tested directly via the inference.py module:

from inference import ensemble_predict_file

# 'sample.wav' must be available in the execution path
label, prob, diag = ensemble_predict_file("sample.wav")
print(f"Prediction: {label} (Probability: {prob:.4f})")


ğŸ’¬ Chatbot Interaction Flow

Bot: â€œHow are you feeling today?â€

User: Records a voice reply.

ML Model: Predicts the depression probability for that individual response.

Bot: Asks the next structured interview question.

Final Result: A comprehensive, aggregated prediction (mean probability across all chunks) is returned after all questions have been answered.

ğŸ“¦ Requirements (requirements.txt)

streamlit
audiorecorder
pydub
fastapi
uvicorn
requests
transformers
torch
numpy
librosa
soundfile
scikit-learn
xgboost
joblib


ğŸš€ Future Enhancements

Multimodal Fusion: Integrate Whisper ASR for text features to fuse text and voice modalities.

Affective Computing: Add an Emotion Recognition Module for richer feature extraction.

Real-Time Streaming: Implement WebRTC for true real-time audio transmission.

Deployment: Create Docker containers for easy deployment and host a live HuggingFace Spaces demo.

ğŸ“˜ License

This project is intended for research & academic purposes only.

ğŸ™Œ Acknowledgements

Facebook AI: For the foundational Wav2Vec2 model.

HuggingFace Transformers: For ease of accessing state-of-the-art models.

Scikit-learn and XGBoost: For robust classification tools.

Streamlit and FastAPI: For the modern, efficient web stack.